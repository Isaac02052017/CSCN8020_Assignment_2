# ğŸ§  Assignment 2 â€“ Q-Learning on Taxi-v3

### ğŸ“˜ Course: Reinforcement Learning Programming â€“ CSCN 8020
**Student:** Srinu Babu Rai  
**Student ID:** 8994032  
**Instructor:** David Espinosa Carrillo  
**Institution:** Conestoga College  
**Date:** October 2025  

---

## ğŸš€ Project Overview
This assignment implements the **Q-Learning algorithm** on the **Taxi-v3** environment from **OpenAI Gym**.  
The objective is to train an agent to efficiently navigate the 5Ã—5 taxi grid, pick up passengers, and drop them off at their destinations while minimizing penalties.  

The project explores the effects of **Learning Rate (Î±)** and **Exploration Factor (Ïµ)** on training performance and stability.  

---

## ğŸ§© Problem Definition
- Environment: **Taxi-v3** (Discrete 500-state problem)  
- Actions: 6 (South, North, East, West, Pickup, Dropoff)  
- Rewards:  
  - `+20` for successful drop-off  
  - `-1` per time-step  
  - `-10` for illegal pickup/drop-off  

---

## âš™ï¸ Implementation Details
- **Algorithm:** Q-Learning (off-policy TD control)  
- **Hyperparameters (Base Run):**  
  - Learning Rate (**Î±**) = 0.1  
  - Discount Factor (**Î³**) = 0.9  
  - Exploration Rate (**Ïµ**) = 0.1  
  - Episodes = 5000  
  - Max Steps per Episode = 200  

---

## ğŸ“Š Parameter Experiments
After training the base model, parameter sweeps were performed to observe learning stability and performance:

| Parameter | Tested Values | Observation |
|------------|----------------|-------------|
| **Learning Rate (Î±)** | 0.001, 0.01, 0.1, 0.2 | Î± = 0.2 achieved the best performance (fastest convergence, least negative rewards). |
| **Exploration Factor (Ïµ)** | 0.1, 0.2, 0.3 | Ïµ = 0.1 provided the most stable results; higher exploration delayed convergence. |

---

## ğŸ§® Key Metrics

| Configuration | Avg Steps | Avg Return | Episodes |
|----------------|------------|-------------|-----------|
| Base (Î±=0.1, Ïµ=0.1) | 30.14 | âˆ’21.15 | 5000 |
| Î± = 0.001 | 185.35 | âˆ’258.64 | 5000 |
| Î± = 0.01 | 127.27 | âˆ’160.73 | 5000 |
| Î± = 0.2 | 23.44 | âˆ’11.41 | 5000 |
| Ïµ = 0.2 | 32.85 | âˆ’32.72 | 5000 |
| Ïµ = 0.3 | 35.86 | âˆ’46.86 | 5000 |

âœ… **Best combination:**  
**Learning Rate Î± = 0.2, Exploration Factor Ïµ = 0.1**  
â†’ *Achieved the highest average return and lowest average steps.*

---

## ğŸ“ˆ Generated Graphs
The notebook produces 8â€“9 key plots:
1. Return per Episode (Base Run)  
2. Steps per Episode (Base Run)  
3. Moving Average of Return  
4. Average Return â€“ All Configurations  
5. Average Steps â€“ All Configurations  
6. Learning Rate (Î±) Sweep â€“ Average Return  
7. Exploration Rate (Ïµ) Sweep â€“ Average Return  
8. Return vs Steps Scatter  
9. (Optional) Exploration Rate (Ïµ) Sweep â€“ Steps  

---

## ğŸ§° Repository Contents
```
Assignment2/
â”‚
â”œâ”€â”€ Assignment 2.ipynb          # Main Jupyter notebook with full code
â”œâ”€â”€ Assignment 2.html           # HTML export of notebook with outputs and plots
â”œâ”€â”€ assignment2_utils.py        # Helper functions for environment and agent setup
â”œâ”€â”€ Assignment2.pdf             # Final formatted report (generated)
â”œâ”€â”€ Assignment2.docx            # Editable Word version of the report
â”œâ”€â”€ requirements.txt            # Dependency file for environment setup
â””â”€â”€ README.md                   # Project overview and instructions
```

---

## ğŸ§‘â€ğŸ’» How to Run

1. **Clone or extract the project folder.**  
   ```bash
   git clone <repository-url>
   cd Assignment2
   ```

2. **Create and activate a virtual environment.**  
   ```bash
   python -m venv .venv
   source .venv/bin/activate   # (macOS/Linux)
   .venv\Scripts\activate      # (Windows)
   ```

3. **Install dependencies.**  
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the notebook.**  
   ```bash
   jupyter notebook "Assignment 2.ipynb"
   ```

5. **View HTML output (optional).**  
   Open `Assignment 2.html` in any web browser.

---

## ğŸ§© Results Summary
- The Q-Learning agent successfully learned optimal routes in the Taxi-v3 grid.  
- Higher learning rates improved training speed and policy accuracy.  
- Excessive exploration (Ïµ â‰¥ 0.2) slowed convergence and reduced reward consistency.  
- **Optimal configuration (Î±=0.2, Ïµ=0.1)** achieved the **best return (âˆ’11.41)** and **lowest average steps (23.44)**.

---

## ğŸ Conclusion
The experiment demonstrates that parameter tuning significantly impacts Q-Learning performance.  
For discrete environments like Taxi-v3, **a moderately high learning rate** and **low exploration rate** yield the most stable and efficient learning outcomes.

---

## ğŸ“š References
- OpenAI Gym Documentation: [https://gymnasium.farama.org/](https://gymnasium.farama.org/)  
- Q-Learning Lecture Notes â€“ Conestoga College (David Espinosa Carrillo)  
- Raju, A. (2024). *Implementing Q-Learning in Python: Taxi Environment Case Study.* Medium.  
